{
  "hash": "2e96a654639b77a42fd8d372ff37b76e",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Lab 02 - Linear & Logistic Regression\"\njupyter: python3\n---\n\n\n\n\n## 목표\n\n- **선형 회귀**: 최소제곱법으로 직선 피팅하기\n- **퍼셉트론**: AND / OR / XOR 게이트 실험 (선형 분리 가능 vs 불가능)\n- **로지스틱 회귀**: 시그모이드, 결정 경계(decision boundary) 시각화\n\n---\n\n## 1. 선형 회귀 (1변수)\n\n$y \\approx w_0 + w_1 x$ 형태. 정규방정식 $\\hat{w} = (X^T X)^{-1} X^T y$ 로 한 번에 구할 수 있다.\n\n::: {#4c146310 .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n```\n:::\n\n\n::: {#285b30b0 .cell execution_count=2}\n``` {.python .cell-code}\n# 데이터: y = 2 + 3x + 노이즈\nnp.random.seed(42)\nn = 30\nx = np.linspace(0, 2, n)\ny = 2 + 3 * x + 0.5 * np.random.randn(n)\n```\n:::\n\n\n::: {#ec7d9341 .cell execution_count=3}\n``` {.python .cell-code}\n# 설계 행렬 X: 각 행이 [1, x_i]\nX = np.column_stack([np.ones(n), x])\nw_hat = np.linalg.solve(X.T @ X, X.T @ y)\nprint(\"w_0 =\", w_hat[0], \", w_1 =\", w_hat[1])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nw_0 = 2.1606891797132195 , w_1 = 2.745237372361261\n```\n:::\n:::\n\n\n::: {#8cae1c0d .cell execution_count=4}\n``` {.python .cell-code}\nplt.scatter(x, y, label=\"data\")\nx_line = np.linspace(0, 2, 100)\ny_line = w_hat[0] + w_hat[1] * x_line\nplt.plot(x_line, y_line, \"r-\", label=\"fitted\")\nplt.xlabel(\"x\"); plt.ylabel(\"y\"); plt.legend(); plt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](lab02-linear-logistic-regression_files/figure-html/cell-5-output-1.png){}\n:::\n:::\n\n\n---\n\n## 2. 퍼셉트론 (AND, OR, XOR)\n\n퍼셉트론: $f(x) = \\mathbb{1}[ w^T x + b > 0 ]$. 선형 분리 가능한 문제만 풀 수 있다.\n\n::: {#5a93ed1b .cell execution_count=5}\n``` {.python .cell-code}\n# AND, OR, XOR 진리표 (입력 2개)\nX_gate = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\ny_and = np.array([0, 0, 0, 1])\ny_or  = np.array([0, 1, 1, 1])\ny_xor = np.array([0, 1, 1, 0])\n```\n:::\n\n\n::: {#e4a75738 .cell execution_count=6}\n``` {.python .cell-code}\ndef perceptron_predict(X, w, b):\n    \"\"\" X: (n, d), w: (d,), b: scalar \"\"\"\n    return (X @ w + b > 0).astype(int)\n\ndef fit_perceptron(X, y, lr=0.1, epochs=100):\n    n, d = X.shape\n    w, b = np.zeros(d), 0.0\n    for _ in range(epochs):\n        pred = perceptron_predict(X, w, b)\n        err = y - pred\n        w += lr * (X.T @ err)\n        b += lr * err.sum()\n    return w, b\n```\n:::\n\n\n::: {#5723172b .cell execution_count=7}\n``` {.python .cell-code}\nw_and, b_and = fit_perceptron(X_gate, y_and)\nw_or,  b_or  = fit_perceptron(X_gate, y_or)\nprint(\"AND 예측:\", perceptron_predict(X_gate, w_and, b_and), \" (정답:\", y_and, \")\")\nprint(\"OR  예측:\", perceptron_predict(X_gate, w_or,  b_or),  \" (정답:\", y_or, \")\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAND 예측: [0 0 0 1]  (정답: [0 0 0 1] )\nOR  예측: [0 1 1 1]  (정답: [0 1 1 1] )\n```\n:::\n:::\n\n\n::: {#8f60c9c7 .cell execution_count=8}\n``` {.python .cell-code}\n# XOR: 선형 분리 불가능 → 퍼셉트론 한 개로는 학습 실패\nw_xor, b_xor = fit_perceptron(X_gate, y_xor)\nprint(\"XOR 예측:\", perceptron_predict(X_gate, w_xor, b_xor), \" (정답:\", y_xor, \")\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nXOR 예측: [0 0 0 0]  (정답: [0 1 1 0] )\n```\n:::\n:::\n\n\n---\n\n## 3. 결정 경계 시각화 (AND / OR)\n\n$w_1 x_1 + w_2 x_2 + b = 0$ 이 결정 경계(직선)이다. $x_2 = -(w_1/w_2)x_1 - b/w_2$.\n\n::: {#e1cbb50c .cell execution_count=9}\n``` {.python .cell-code}\ndef plot_decision_boundary_2d(X, y, w, b, title=\"\"):\n    plt.scatter(X[y==0, 0], X[y==0, 1], marker=\"o\", label=\"0\")\n    plt.scatter(X[y==1, 0], X[y==1, 1], marker=\"s\", label=\"1\")\n    x1 = np.array([-0.5, 1.5])\n    if np.abs(w[1]) > 1e-8:\n        x2 = -(w[0] * x1 + b) / w[1]\n        plt.plot(x1, x2, \"k-\", label=\"decision boundary\")\n    plt.xlabel(\"$x_1$\"); plt.ylabel(\"$x_2$\"); plt.legend(); plt.title(title); plt.show()\n```\n:::\n\n\n::: {#c030117e .cell execution_count=10}\n``` {.python .cell-code}\nplot_decision_boundary_2d(X_gate, y_and, w_and, b_and, \"AND\")\n```\n\n::: {.cell-output .cell-output-display}\n![](lab02-linear-logistic-regression_files/figure-html/cell-11-output-1.png){}\n:::\n:::\n\n\n::: {#42ae028a .cell execution_count=11}\n``` {.python .cell-code}\nplot_decision_boundary_2d(X_gate, y_or, w_or, b_or, \"OR\")\n```\n\n::: {.cell-output .cell-output-display}\n![](lab02-linear-logistic-regression_files/figure-html/cell-12-output-1.png){}\n:::\n:::\n\n\n---\n\n## 4. 로지스틱 회귀 (결정 경계)\n\n로지스틱 회귀: $P(y=1|x) = \\sigma(w^T x + b)$, $\\sigma(z) = 1/(1+e^{-z})$.  \n결정 경계는 $\\sigma=0.5$ 인 곳, 즉 $w^T x + b = 0$ (직선).\n\n::: {#19eb97ca .cell execution_count=12}\n``` {.python .cell-code}\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n\n# 2D 예시 데이터: 두 클래스가 약간 겹침\nnp.random.seed(1)\nn1, n2 = 25, 25\nX1 = np.random.randn(n1, 2) + np.array([1, 1])\nX2 = np.random.randn(n2, 2) + np.array([-1, -1])\nX_lr = np.vstack([X1, X2])\ny_lr = np.array([0] * n1 + [1] * n2)\n```\n:::\n\n\n::: {#34b51e35 .cell execution_count=13}\n``` {.python .cell-code}\n# 간단한 경사 하강으로 w, b 추정 (음의 로그우도 최소화)\ndef fit_logistic_simple(X, y, lr=0.1, epochs=2000):\n    n, d = X.shape\n    w, b = np.zeros(d), 0.0\n    for _ in range(epochs):\n        logit = X @ w + b\n        p = sigmoid(logit)\n        grad_w = X.T @ (p - y)\n        grad_b = (p - y).sum()\n        w -= lr * grad_w\n        b -= lr * grad_b\n    return w, b\n\nw_lr, b_lr = fit_logistic_simple(X_lr, y_lr)\npred_lr = (sigmoid(X_lr @ w_lr + b_lr) >= 0.5).astype(int)\nprint(\"정확도:\", (pred_lr == y_lr).mean())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n정확도: 0.98\n```\n:::\n:::\n\n\n::: {#3d841452 .cell execution_count=14}\n``` {.python .cell-code}\n# 결정 경계 그리기: w1*x1 + w2*x2 + b = 0  →  x2 = -(w1*x1 + b)/w2\nplt.scatter(X_lr[y_lr==0, 0], X_lr[y_lr==0, 1], marker=\"o\", label=\"0\")\nplt.scatter(X_lr[y_lr==1, 0], X_lr[y_lr==1, 1], marker=\"s\", label=\"1\")\nx1_min, x1_max = X_lr[:, 0].min() - 0.5, X_lr[:, 0].max() + 0.5\nx1_grid = np.linspace(x1_min, x1_max, 100)\nif np.abs(w_lr[1]) > 1e-8:\n    x2_boundary = -(w_lr[0] * x1_grid + b_lr) / w_lr[1]\n    plt.plot(x1_grid, x2_boundary, \"k-\", lw=2, label=\"decision boundary\")\nplt.xlabel(\"$x_1$\"); plt.ylabel(\"$x_2$\"); plt.legend(); plt.title(\"Logistic regression\"); plt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](lab02-linear-logistic-regression_files/figure-html/cell-15-output-1.png){}\n:::\n:::\n\n\n---\n\n## 5. 연습\n\n- XOR 데이터에 대해 **두 개의 퍼셉트론**을 써서 해결하는 방법을 생각해 보세요.  \n  (힌트: 은닉층 하나를 두고 AND(OR, OR) 같은 구조.)\n- 위 로지스틱 회귀에서 `epochs`를 100으로 줄이면 결정 경계가 어떻게 달라지는지 확인해 보세요.\n\n---\n\n## 참고\n\n- 퍼셉트론 하나 = 선형 결정 경계만 가능. XOR은 다층 퍼셉트론(MLP)으로 해결.\n- 로지스틱 회귀의 결정 경계도 선형; 비선형 경계가 필요하면 특성 추가 또는 신경망을 사용한다.\n\n",
    "supporting": [
      "lab02-linear-logistic-regression_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}