{
  "hash": "e9ba9e1eba47cce7ba30c2bb5e96b3a8",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Lab 01 - NumPy Basics for Machine Learning\"\njupyter: python3\n---\n\n\n\n\n[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/GLI-Lab/machine-learning-course/blob/students/exercises/lab01/numpy-basics.ipynb)\n\n## Objectives\n\n- Understand the basic concepts of NumPy arrays: creation, `shape`, and `dtype`.\n- **Vectorization**: Learn how to replace slow Python loops with fast, C-optimized array operations.\n- **Broadcasting**: Grasp the rules NumPy uses to perform operations on arrays of different shapes.\n\n::: {.callout-note}\n## Basic Concept: Why NumPy?\nIn Machine Learning, we deal with massive amounts of data (vectors, matrices, tensors). Standard Python lists are too slow and memory-inefficient for these calculations. NumPy arrays are stored in contiguous blocks of memory and operations are implemented in highly optimized C code, making mathematical operations drastically faster. NumPy is the foundational library that modern deep learning frameworks (like PyTorch and TensorFlow) are built upon.\n:::\n\n## 1. Array Creation and Attributes\n\nBefore we manipulate data, we need to know how to create arrays and inspect their properties.\n\n::: {#c975d16e .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\n```\n:::\n\n\n::: {#1e1aa830 .cell execution_count=2}\n``` {.python .cell-code}\n# 1-Dimensional Array (Vector)\na = np.array([1, 2, 3, 4, 5])\nprint(\"a =\", a)\nprint(\"shape:\", a.shape) # Returns a tuple representing the size of each dimension\nprint(\"dtype:\", a.dtype) # Data type of the elements\n\nb = np.array([1.0, 2.0, 3.0, 4.0, 5.0])\nprint(\"b =\", b)\nprint(\"shape:\", b.shape) # Returns a tuple representing the size of each dimension\nprint(\"dtype:\", b.dtype) # Data type of the elements\n```\n\n::: {.cell-output .cell-output-stdout}\n```\na = [1 2 3 4 5]\nshape: (5,)\ndtype: int64\nb = [1. 2. 3. 4. 5.]\nshape: (5,)\ndtype: float64\n```\n:::\n:::\n\n\n::: {#8d37c5f5 .cell execution_count=3}\n``` {.python .cell-code}\n# 2-Dimensional Array (Matrix)\nB = np.array([[1, 2], [3, 4], [5, 6]])\nprint(\"B =\\n\", B)\nprint(\"shape:\", B.shape)  # (number of rows, number of columns)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nB =\n [[1 2]\n [3 4]\n [5 6]]\nshape: (3, 2)\n```\n:::\n:::\n\n\n::: {.callout-warning}\n\n## Caveat: Data Types (dtype)\n\nBy default, NumPy might create arrays with `float64` or `int64` depending on your OS and the input. In deep learning, memory is expensive, so we often explicitly cast arrays to 32-bit floats using `a.astype(np.float32)`.\n:::\n\n::: {#d5ebbda3 .cell execution_count=4}\n``` {.python .cell-code}\n# Frequently used creation functions\nprint(\"Zeros:\\n\", np.zeros(5))               # Array of zeros\nprint(\"Ones:\\n\", np.ones((2, 3)))            # 2x3 matrix of ones\nprint(\"Arange:\\n\", np.arange(0, 10, 2))      # start, stop (exclusive), step\nprint(\"Linspace:\\n\", np.linspace(0, 1, 5))   # 5 evenly spaced numbers between 0 and 1 (inclusive)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nZeros:\n [0. 0. 0. 0. 0.]\nOnes:\n [[1. 1. 1.]\n [1. 1. 1.]]\nArange:\n [0 2 4 6 8]\nLinspace:\n [0.   0.25 0.5  0.75 1.  ]\n```\n:::\n:::\n\n\n## 2. Indexing and Slicing\n\nAccessing elements in NumPy is similar to standard Python lists but extends to multiple dimensions.\n\n::: {#af47fe0a .cell execution_count=5}\n``` {.python .cell-code}\nx = np.array([10, 20, 30, 40, 50])\nprint(\"First element:\", x[0])\nprint(\"Last element:\", x[-1])\nprint(\"Slice [1:4]:\", x[1:4])     # Indices 1, 2, 3 (exclusive of 4)\nprint(\"Step slice [::2]:\", x[::2]) # Every 2nd element from start to end\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFirst element: 10\nLast element: 50\nSlice [1:4]: [20 30 40]\nStep slice [::2]: [10 30 50]\n```\n:::\n:::\n\n\n::: {#5d2b184e .cell execution_count=6}\n``` {.python .cell-code}\nM = np.arange(12).reshape(3, 4)\nprint(\"M =\\n\", M)\nprint(\"Specific element M[1, 2]:\", M[1, 2]) # Row 1, Column 2 (0-indexed)\nprint(\"Entire 2nd row:\", M[2, :])\nprint(\"Entire 3rd column:\", M[:, 3])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nM =\n [[ 0  1  2  3]\n [ 4  5  6  7]\n [ 8  9 10 11]]\nSpecific element M[1, 2]: 6\nEntire 2nd row: [ 8  9 10 11]\nEntire 3rd column: [ 3  7 11]\n```\n:::\n:::\n\n\n::: {.callout-warning}\n\n## Caveat: Slices are Views, Not Copies!\n\nWhen you slice an array in NumPy, it returns a **view** of the original memory, not a new array. If you modify the slice, the original array is modified too. If you need an independent copy, explicitly use `.copy()`.\n\n```python\nx_slice = x[1:4].copy() \n\n```\n\n:::\n\n## 3. Vectorization\n\nVectorization is the process of applying an operation to an entire array at once, rather than iterating through its elements using a `for` loop. This results in cleaner code and massive performance boosts.\n\n::: {#c3358eea .cell execution_count=7}\n``` {.python .cell-code}\n# Non-vectorized approach: Python 'for' loop\ndef sum_squares_loop(arr):\n    total = 0\n    for x in arr:\n        total += x ** 2\n    return total\n\n# Vectorized approach: NumPy array operation\ndef sum_squares_vec(arr):\n    return (arr ** 2).sum()\n```\n:::\n\n\nLet's test the speed difference:\n\n::: {#05045282 .cell execution_count=8}\n``` {.python .cell-code}\nlarge = np.random.randn(100_000)\nprint(\"Loop time:\")\n%timeit sum_squares_loop(large)\nprint(\"Vectorized time:\")\n%timeit sum_squares_vec(large)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLoop time:\n8.56 ms ± 51.5 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\nVectorized time:\n24.5 μs ± 242 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n```\n:::\n:::\n\n\nVectorization makes applying mathematical functions to arrays trivial. For example, implementing the Sigmoid activation function $\\sigma(z) = \\frac{1}{1 + e^{-z}}$:\n\n::: {#0bac35a8 .cell execution_count=9}\n``` {.python .cell-code}\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n\nz = np.linspace(-5, 5, 11)\nprint(\"z          =\", z)\nprint(\"sigmoid(z) =\", sigmoid(z))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nz          = [-5. -4. -3. -2. -1.  0.  1.  2.  3.  4.  5.]\nsigmoid(z) = [0.00669285 0.01798621 0.04742587 0.11920292 0.26894142 0.5\n 0.73105858 0.88079708 0.95257413 0.98201379 0.99330715]\n```\n:::\n:::\n\n\n## 4. Broadcasting\n\nBroadcasting is a powerful mechanism that allows NumPy to perform operations on arrays of different shapes.\n\n**The Rule of Broadcasting:** NumPy compares array shapes from right to left. Two dimensions are compatible if:\n\n1. They are equal, or\n2. One of them is 1.\nIf an array has fewer dimensions, a dimension of `1` is implicitly prepended to its shape.\n\n::: {#70407f19 .cell execution_count=10}\n``` {.python .cell-code}\n# Shape (3, 4) + Shape (4,) \n# The (4,) is treated as (1, 4), then copied across 3 rows to match (3, 4)\nA = np.arange(12).reshape(3, 4)\nb = np.array([1, 2, 3, 4])\nprint(\"A =\\n\", A)\nprint(\"b =\", b)\nprint(\"A + b =\\n\", A + b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nA =\n [[ 0  1  2  3]\n [ 4  5  6  7]\n [ 8  9 10 11]]\nb = [1 2 3 4]\nA + b =\n [[ 1  3  5  7]\n [ 5  7  9 11]\n [ 9 11 13 15]]\n```\n:::\n:::\n\n\n::: {#67f348ec .cell execution_count=11}\n``` {.python .cell-code}\n# Shape (3, 4) + Shape (3, 1)\n# The (3, 1) array is copied across 4 columns to match (3, 4)\nc = np.array([[10], [20], [30]])\nprint(\"c =\\n\", c)\nprint(\"A + c =\\n\", A + c)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nc =\n [[10]\n [20]\n [30]]\nA + c =\n [[10 11 12 13]\n [24 25 26 27]\n [38 39 40 41]]\n```\n:::\n:::\n\n\n**Practical Example: Normalizing Data**\nA very common task in ML is normalizing data (e.g., making rows sum to 1).\n\n::: {#4f31cc22 .cell execution_count=12}\n``` {.python .cell-code}\n# keepdims=True ensures the shape remains (3, 1) instead of dropping to (3,)\nrow_sums = A.sum(axis=1, keepdims=True)  \nprint(\"row_sums (shape\", row_sums.shape, \") =\\n\", row_sums)\n\nnormalized = A / row_sums\nprint(\"Normalized A (rows sum to 1):\\n\", normalized)\nprint(\"Verify row sums:\", normalized.sum(axis=1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nrow_sums (shape (3, 1) ) =\n [[ 6]\n [22]\n [38]]\nNormalized A (rows sum to 1):\n [[0.         0.16666667 0.33333333 0.5       ]\n [0.18181818 0.22727273 0.27272727 0.31818182]\n [0.21052632 0.23684211 0.26315789 0.28947368]]\nVerify row sums: [1. 1. 1.]\n```\n:::\n:::\n\n\n::: {.callout-warning}\n\n## Caveat: The Danger of Rank-1 Arrays\n\nArrays with shape `(N,)` are called Rank-1 arrays (e.g., `b` in the example above). They do not behave like traditional row or column vectors and can cause confusing broadcasting bugs.\n**Best Practice in ML:** Always reshape your 1D vectors into explicit 2D row matrices `(1, N)` or column matrices `(N, 1)` using `.reshape(-1, 1)` or `np.newaxis` to avoid unintended broadcasting behaviors.\n:::\n\n## 5. Image Feature Extraction\n\nA single photo (e.g. of a house or street) is just a NumPy array: **grayscale** images have shape `(height, width)` and **color** images `(height, width, 3)` for red, green, and blue. We will load both versions of the same scene, compare how they are stored, then use **sliding local windows** over the grayscale image to build a simple blur—each output pixel is the mean of a small square of input pixels. The same idea (local windows over a grid) appears in time series (rolling averages) and in convolutional layers in deep learning.\n\n### Loading the image: grayscale and color\n\nWe use two files: a grayscale PNG and an RGB PNG of the same scene. The image is of the Lorch Monastery church; the original photograph is available at [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Lorch_(W.)_-_Kloster_Lorch_-_Klosterkirche_-_Ansicht_von_SO_(1).jpg). Grayscale images give one number per pixel, while RGB images give three numbers per pixel (one for each of red, green, and blue). If your grayscale file is instead stored as three identical channels (R=G=B), we collapse it to 2D so we have a single intensity per pixel.\n\n::: {#a56970e0 .cell execution_count=13}\n``` {.python .cell-code}\nimport matplotlib.image as mpimg\nimport matplotlib.pyplot as plt\n\nimg_gray = mpimg.imread(\"../../data/image_gray.png\")\nimg_rgb = mpimg.imread(\"../../data/image_rgb.png\")\n\nprint(\"Grayscale shape :\", img_gray.shape, \"→ one value per pixel\")\nprint(\"RGB shape       :\", img_rgb.shape, \"→ three values per pixel\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGrayscale shape : (317, 500) → one value per pixel\nRGB shape       : (317, 500, 3) → three values per pixel\n```\n:::\n:::\n\n\n::: {#ce27c7dd .cell execution_count=14}\n``` {.python .cell-code}\nfig, axes = plt.subplots(1, 2, figsize=(10, 4))\naxes[0].imshow(img_gray, cmap='gray')\naxes[0].set_title(\"Grayscale\")\naxes[0].axis(\"off\")\naxes[1].imshow(img_rgb)\naxes[1].set_title(\"RGB\")\naxes[1].axis(\"off\")\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](numpy-basics_files/figure-html/cell-15-output-1.png){}\n:::\n:::\n\n\n### Local windows and simple blur\n\nReplace each pixel by the **mean** of a small square (e.g. 10×10) around it. That gives a blurred image: edges soften and noise is reduced. We do this by sweeping a **sliding window** over the array—same idea as a rolling mean along a 1D signal, but in 2D.\n\nA 3×3 window in the top-left corner of the grayscale image looks like this:\n\n::: {#44d6dfbf .cell execution_count=15}\n``` {.python .cell-code}\ntop_left = img_gray[:3, :3]\nprint(\"Top-left 3×3 block:\\n\", top_left)\nprint(\"Mean of this block:\", top_left.mean())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTop-left 3×3 block:\n [[0.85882354 0.85882354 0.85882354]\n [0.83137256 0.83137256 0.8352941 ]\n [0.8235294  0.8235294  0.827451  ]]\nMean of this block: 0.83878\n```\n:::\n:::\n\n\n**Double loop:** For every possible top-left corner `(i, j)` on the **full image**, take the block `img[i:i+size, j:j+size]`, compute its mean, and store it. Clear, but many Python iterations.\n\n::: {#78cf107b .cell execution_count=16}\n``` {.python .cell-code}\nimport timeit\n\nsize = 10\nm, n = img_gray.shape\nmm, nn = m - size + 1, n - size + 1\n\nresult_loop = np.empty((mm, nn))\nfor i in range(mm):\n    for j in range(nn):\n        result_loop[i, j] = img_gray[i : i + size, j : j + size].mean()\n\ndef run_loop():\n    r = np.empty((mm, nn))\n    for i in range(mm):\n        for j in range(nn):\n            r[i, j] = img_gray[i : i + size, j : j + size].mean()\n    return r\nt_loop = timeit.repeat(run_loop, number=1, repeat=10)\nprint(\"Loop:       {:.4f} s (mean of 10 runs)\".format(np.mean(t_loop)))\nprint(\"Blurred shape:\", result_loop.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLoop:       0.4025 s (mean of 10 runs)\nBlurred shape: (308, 491)\n```\n:::\n:::\n\n\n**Vectorized view with `as_strided`:** NumPy’s `stride_tricks.as_strided` lets you reinterpret the same memory as a 4D array of overlapping windows over the **full image**. No extra memory for the tiles; we only change how we index into the array. Then we take the mean over the last two axes to get one number per window.\n\n::: {#bdd0e507 .cell execution_count=17}\n``` {.python .cell-code}\nfrom numpy.lib import stride_tricks\n\nshape = (mm, nn, size, size)\nstrides = 2 * img_gray.strides\nwindows = stride_tricks.as_strided(img_gray, shape=shape, strides=strides)\nresult_vec = windows.mean(axis=(-1, -2))\n\ndef run_vec():\n    w = stride_tricks.as_strided(img_gray, shape=shape, strides=strides)\n    return w.mean(axis=(-1, -2))\nt_vec = timeit.repeat(run_vec, number=1, repeat=10)\nprint(\"Vectorized: {:.4f} s (mean of 10 runs)\".format(np.mean(t_vec)))\nprint(\"Same as loop:\", np.allclose(result_loop, result_vec))\nprint(\"Speedup:     {:.1f}x\".format(np.mean(t_loop) / np.mean(t_vec)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nVectorized: 0.0189 s (mean of 10 runs)\nSame as loop: True\nSpeedup:     21.2x\n```\n:::\n:::\n\n\n**Output comparison:** Both methods produce the same blurred image over the full image; the vectorized version is typically much faster.\n\n::: {#7f0fa095 .cell execution_count=18}\n``` {.python .cell-code}\nfig, axes = plt.subplots(1, 3, figsize=(12, 4))\naxes[0].imshow(img_gray, cmap='gray')\naxes[0].set_title(\"Original (full image)\")\naxes[0].axis(\"off\")\naxes[1].imshow(result_loop, cmap='gray')\naxes[1].set_title(\"Blurred (loop)\")\naxes[1].axis(\"off\")\naxes[2].imshow(result_vec, cmap='gray')\naxes[2].set_title(\"Blurred (vectorized)\")\naxes[2].axis(\"off\")\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](numpy-basics_files/figure-html/cell-19-output-1.png){}\n:::\n:::\n\n\n::: {.callout-tip}\n## Strides and views\nThe **strides** of an array tell how many bytes to skip when moving by one step along each dimension. `as_strided` only changes the shape and strides; it does not copy data, so you get a new *view* of the same buffer. For production code, libraries like Scikit-Learn provide helpers to extract sliding windows without writing strides by hand.\n:::\n\n## 6. Exercises\n\nCreate a vector `v` and a matrix `M` with shape `(4, 3)` where every row of `M` is exactly `v`.\n*(Hint: You can achieve this using `np.tile` or by utilizing broadcasting).*\n\n::: {#c7cc7862 .cell execution_count=19}\n``` {.python .cell-code}\nv = np.array([1, 2, 3])\n# M = ???  # Desired shape (4, 3), each row is [1, 2, 3]\n# print(M)\n```\n:::\n\n\n**Solutions:**\n\n::: {#69b01d0f .cell execution_count=20}\n``` {.python .cell-code}\n# Solution 1: Using np.tile\nM1 = np.tile(v, (4, 1))\nprint(\"Using tile:\\n\", M1)\n\n# Solution 2: Using broadcasting\n# Adding a (3,) vector to a (4, 3) matrix of zeros automatically broadcasts it.\nM2 = v + np.zeros((4, 3))\nprint(\"Using broadcasting:\\n\", M2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUsing tile:\n [[1 2 3]\n [1 2 3]\n [1 2 3]\n [1 2 3]]\nUsing broadcasting:\n [[1. 2. 3.]\n [1. 2. 3.]\n [1. 2. 3.]\n [1. 2. 3.]]\n```\n:::\n:::\n\n\n## Summary for Machine Learning\n\n* In ML, our datasets are usually represented as 2D arrays with the shape `(num_samples, num_features)`.\n* When calculating gradients, losses, or passing data through neural network layers, **never** use Python loops. Always use **Vectorization** and **Broadcasting**. It will make your code shorter, significantly faster, and easier to read.\n\n## References\n\n- [Python for Data Analysis](https://www.statlearning.com/)\n- [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Lorch_(W.)_-_Kloster_Lorch_-_Klosterkirche_-_Ansicht_von_SO_(1).jpg)\n\n## Further Reading\n\n- [A Whirlwind Tour of Python](https://github.com/jakevdp/WhirlwindTourOfPython/?tab=readme-ov-file)\n- [Python Data Science Handbook](https://github.com/jakevdp/PythonDataScienceHandbook)\n- [Data Science With Python Core Skills](https://realpython.com/learning-paths/data-science-python-core-skills/)\n\n",
    "supporting": [
      "numpy-basics_files"
    ],
    "filters": [],
    "includes": {}
  }
}