{
  "hash": "61840e97f97c5d76bb5fd3289357cfae",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Lab 01 - NumPy Basics for Machine Learning\"\njupyter: python3\n---\n\n\n\n\n[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/GLI-Lab/machine-learning-course/blob/students/exercises/lab01/numpy-basics.ipynb)\n\n## Objectives\n\n- Understand the basic concepts of NumPy arrays: creation, `shape`, and `dtype`.\n- **Vectorization**: Learn how to replace slow Python loops with fast, C-optimized array operations.\n- **Broadcasting**: Grasp the rules NumPy uses to perform operations on arrays of different shapes.\n\n::: {.callout-note}\n## Basic Concept: Why NumPy?\nIn Machine Learning, we deal with massive amounts of data (vectors, matrices, tensors). Standard Python lists are too slow and memory-inefficient for these calculations. NumPy arrays are stored in contiguous blocks of memory and operations are implemented in highly optimized C code, making mathematical operations drastically faster. NumPy is the foundational library that modern deep learning frameworks (like PyTorch and TensorFlow) are built upon.\n:::\n\n## 1. Array Creation and Attributes\n\nBefore we manipulate data, we need to know how to create arrays and inspect their properties.\n\n::: {#9f21541f .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\n```\n:::\n\n\n::: {#00dcfad7 .cell execution_count=2}\n``` {.python .cell-code}\n# 1-Dimensional Array (Vector)\na = np.array([1, 2, 3, 4, 5])\nprint(\"a =\", a)\nprint(\"shape:\", a.shape) # Returns a tuple representing the size of each dimension\nprint(\"dtype:\", a.dtype) # Data type of the elements\n```\n\n::: {.cell-output .cell-output-stdout}\n```\na = [1 2 3 4 5]\nshape: (5,)\ndtype: int64\n```\n:::\n:::\n\n\n::: {#fa8a1c77 .cell execution_count=3}\n``` {.python .cell-code}\n# 2-Dimensional Array (Matrix)\nB = np.array([[1, 2], [3, 4], [5, 6]])\nprint(\"B =\\n\", B)\nprint(\"shape:\", B.shape)  # (number of rows, number of columns)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nB =\n [[1 2]\n [3 4]\n [5 6]]\nshape: (3, 2)\n```\n:::\n:::\n\n\n::: {.callout-warning}\n\n## Caveat: Data Types (dtype)\n\nBy default, NumPy might create arrays with `float64` or `int64` depending on your OS and the input. In deep learning, memory is expensive, so we often explicitly cast arrays to 32-bit floats using `a.astype(np.float32)`.\n:::\n\n::: {#850fbeea .cell execution_count=4}\n``` {.python .cell-code}\n# Frequently used creation functions\nprint(\"Zeros:\\n\", np.zeros(5))               # Array of zeros\nprint(\"Ones:\\n\", np.ones((2, 3)))            # 2x3 matrix of ones\nprint(\"Arange:\\n\", np.arange(0, 10, 2))      # start, stop (exclusive), step\nprint(\"Linspace:\\n\", np.linspace(0, 1, 5))   # 5 evenly spaced numbers between 0 and 1 (inclusive)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nZeros:\n [0. 0. 0. 0. 0.]\nOnes:\n [[1. 1. 1.]\n [1. 1. 1.]]\nArange:\n [0 2 4 6 8]\nLinspace:\n [0.   0.25 0.5  0.75 1.  ]\n```\n:::\n:::\n\n\n## 2. Indexing and Slicing\n\nAccessing elements in NumPy is similar to standard Python lists but extends to multiple dimensions.\n\n::: {#4004b23f .cell execution_count=5}\n``` {.python .cell-code}\nx = np.array([10, 20, 30, 40, 50])\nprint(\"First element:\", x[0])\nprint(\"Last element:\", x[-1])\nprint(\"Slice [1:4]:\", x[1:4])     # Indices 1, 2, 3 (exclusive of 4)\nprint(\"Step slice [::2]:\", x[::2]) # Every 2nd element from start to end\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFirst element: 10\nLast element: 50\nSlice [1:4]: [20 30 40]\nStep slice [::2]: [10 30 50]\n```\n:::\n:::\n\n\n::: {#85491645 .cell execution_count=6}\n``` {.python .cell-code}\nM = np.arange(12).reshape(3, 4)\nprint(\"M =\\n\", M)\nprint(\"Specific element M[1, 2]:\", M[1, 2]) # Row 1, Column 2 (0-indexed)\nprint(\"Entire 2nd row:\", M[2, :])\nprint(\"Entire 3rd column:\", M[:, 3])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nM =\n [[ 0  1  2  3]\n [ 4  5  6  7]\n [ 8  9 10 11]]\nSpecific element M[1, 2]: 6\nEntire 2nd row: [ 8  9 10 11]\nEntire 3rd column: [ 3  7 11]\n```\n:::\n:::\n\n\n::: {.callout-warning}\n\n## Caveat: Slices are Views, Not Copies!\n\nWhen you slice an array in NumPy, it returns a **view** of the original memory, not a new array. If you modify the slice, the original array is modified too. If you need an independent copy, explicitly use `.copy()`.\n\n```python\nx_slice = x[1:4].copy() \n\n```\n\n:::\n\n## 3. Vectorization\n\nVectorization is the process of applying an operation to an entire array at once, rather than iterating through its elements using a `for` loop. This results in cleaner code and massive performance boosts.\n\n::: {#40e169e7 .cell execution_count=7}\n``` {.python .cell-code}\n# Non-vectorized approach: Python 'for' loop\ndef sum_squares_loop(arr):\n    total = 0\n    for x in arr:\n        total += x ** 2\n    return total\n\n# Vectorized approach: NumPy array operation\ndef sum_squares_vec(arr):\n    return (arr ** 2).sum()\n```\n:::\n\n\nLet's test the speed difference:\n\n::: {#b2aedd41 .cell execution_count=8}\n``` {.python .cell-code}\nlarge = np.random.randn(100_000)\nprint(\"Loop time:\")\n%timeit sum_squares_loop(large)\nprint(\"Vectorized time:\")\n%timeit sum_squares_vec(large)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLoop time:\n8.37 ms ± 48.7 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\nVectorized time:\n24.2 μs ± 22.4 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n```\n:::\n:::\n\n\nVectorization makes applying mathematical functions to arrays trivial. For example, implementing the Sigmoid activation function $\\sigma(z) = \\frac{1}{1 + e^{-z}}$:\n\n::: {#5c0f837d .cell execution_count=9}\n``` {.python .cell-code}\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n\nz = np.linspace(-5, 5, 11)\nprint(\"z          =\", z)\nprint(\"sigmoid(z) =\", sigmoid(z))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nz          = [-5. -4. -3. -2. -1.  0.  1.  2.  3.  4.  5.]\nsigmoid(z) = [0.00669285 0.01798621 0.04742587 0.11920292 0.26894142 0.5\n 0.73105858 0.88079708 0.95257413 0.98201379 0.99330715]\n```\n:::\n:::\n\n\n## 4. Broadcasting\n\nBroadcasting is a powerful mechanism that allows NumPy to perform operations on arrays of different shapes.\n\n**The Rule of Broadcasting:** NumPy compares array shapes from right to left. Two dimensions are compatible if:\n\n1. They are equal, or\n2. One of them is 1.\nIf an array has fewer dimensions, a dimension of `1` is implicitly prepended to its shape.\n\n::: {#ac313818 .cell execution_count=10}\n``` {.python .cell-code}\n# Shape (3, 4) + Shape (4,) \n# The (4,) is treated as (1, 4), then copied across 3 rows to match (3, 4)\nA = np.arange(12).reshape(3, 4)\nb = np.array([1, 2, 3, 4])\nprint(\"A =\\n\", A)\nprint(\"b =\", b)\nprint(\"A + b =\\n\", A + b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nA =\n [[ 0  1  2  3]\n [ 4  5  6  7]\n [ 8  9 10 11]]\nb = [1 2 3 4]\nA + b =\n [[ 1  3  5  7]\n [ 5  7  9 11]\n [ 9 11 13 15]]\n```\n:::\n:::\n\n\n::: {#24845975 .cell execution_count=11}\n``` {.python .cell-code}\n# Shape (3, 4) + Shape (3, 1)\n# The (3, 1) array is copied across 4 columns to match (3, 4)\nc = np.array([[10], [20], [30]])\nprint(\"c =\\n\", c)\nprint(\"A + c =\\n\", A + c)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nc =\n [[10]\n [20]\n [30]]\nA + c =\n [[10 11 12 13]\n [24 25 26 27]\n [38 39 40 41]]\n```\n:::\n:::\n\n\n**Practical Example: Normalizing Data**\nA very common task in ML is normalizing data (e.g., making rows sum to 1).\n\n::: {#e1398ea8 .cell execution_count=12}\n``` {.python .cell-code}\n# keepdims=True ensures the shape remains (3, 1) instead of dropping to (3,)\nrow_sums = A.sum(axis=1, keepdims=True)  \nprint(\"row_sums (shape\", row_sums.shape, \") =\\n\", row_sums)\n\nnormalized = A / row_sums\nprint(\"Normalized A (rows sum to 1):\\n\", normalized)\nprint(\"Verify row sums:\", normalized.sum(axis=1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nrow_sums (shape (3, 1) ) =\n [[ 6]\n [22]\n [38]]\nNormalized A (rows sum to 1):\n [[0.         0.16666667 0.33333333 0.5       ]\n [0.18181818 0.22727273 0.27272727 0.31818182]\n [0.21052632 0.23684211 0.26315789 0.28947368]]\nVerify row sums: [1. 1. 1.]\n```\n:::\n:::\n\n\n::: {.callout-warning}\n\n## Caveat: The Danger of Rank-1 Arrays\n\nArrays with shape `(N,)` are called Rank-1 arrays (e.g., `b` in the example above). They do not behave like traditional row or column vectors and can cause confusing broadcasting bugs.\n**Best Practice in ML:** Always reshape your 1D vectors into explicit 2D row matrices `(1, N)` or column matrices `(N, 1)` using `.reshape(-1, 1)` or `np.newaxis` to avoid unintended broadcasting behaviors.\n:::\n\n## 5. Exercises\n\nCreate a vector `v` and a matrix `M` with shape `(4, 3)` where every row of `M` is exactly `v`.\n*(Hint: You can achieve this using `np.tile` or by utilizing broadcasting).*\n\n::: {#d4802f69 .cell execution_count=13}\n``` {.python .cell-code}\nv = np.array([1, 2, 3])\n# M = ???  # Desired shape (4, 3), each row is [1, 2, 3]\n# print(M)\n```\n:::\n\n\n**Solutions:**\n\n::: {#dc16f841 .cell execution_count=14}\n``` {.python .cell-code}\n# Solution 1: Using np.tile\nM1 = np.tile(v, (4, 1))\nprint(\"Using tile:\\n\", M1)\n\n# Solution 2: Using broadcasting\n# Adding a (3,) vector to a (4, 3) matrix of zeros automatically broadcasts it.\nM2 = v + np.zeros((4, 3))\nprint(\"Using broadcasting:\\n\", M2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUsing tile:\n [[1 2 3]\n [1 2 3]\n [1 2 3]\n [1 2 3]]\nUsing broadcasting:\n [[1. 2. 3.]\n [1. 2. 3.]\n [1. 2. 3.]\n [1. 2. 3.]]\n```\n:::\n:::\n\n\n## Summary for Machine Learning\n\n* In ML, our datasets are usually represented as 2D arrays with the shape `(num_samples, num_features)`.\n* When calculating gradients, losses, or passing data through neural network layers, **never** use Python loops. Always use **Vectorization** and **Broadcasting**. It will make your code shorter, significantly faster, and easier to read.\n\n```\n\nWould you like me to add a section specifically covering Matrix Multiplication (`np.dot` vs `@`), as that is also a crucial NumPy concept for Machine Learning?\n\n```\n\n",
    "supporting": [
      "numpy-basics_files"
    ],
    "filters": [],
    "includes": {}
  }
}