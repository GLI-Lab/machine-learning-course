[
  {
    "objectID": "assignments/hw08-decision-tree-ensemble.html",
    "href": "assignments/hw08-decision-tree-ensemble.html",
    "title": "Decision Tree & Ensemble",
    "section": "",
    "text": "Decision Tree 및 Ensemble 방법 실습.",
    "crumbs": [
      "Assignments",
      "Decision Tree & Ensemble"
    ]
  },
  {
    "objectID": "assignments/hw04-pytorch-cnn.html",
    "href": "assignments/hw04-pytorch-cnn.html",
    "title": "CNN",
    "section": "",
    "text": "PyTorch로 convolutional neural network 구현.",
    "crumbs": [
      "Assignments",
      "CNN"
    ]
  },
  {
    "objectID": "assignments/hw03-pytorch-mlp.html",
    "href": "assignments/hw03-pytorch-mlp.html",
    "title": "Feedforward NN",
    "section": "",
    "text": "PyTorch로 feedforward neural network 구현 및 학습.",
    "crumbs": [
      "Assignments",
      "Feedforward NN"
    ]
  },
  {
    "objectID": "assignments/index.html",
    "href": "assignments/index.html",
    "title": "Assignments",
    "section": "",
    "text": "과제 목록입니다. 왼쪽 메뉴에서 항목을 선택하세요.",
    "crumbs": [
      "Assignments"
    ]
  },
  {
    "objectID": "assignments/hw01-environment-setup.html",
    "href": "assignments/hw01-environment-setup.html",
    "title": "Environment setup",
    "section": "",
    "text": "Python / Numpy / Matplotlib 환경 설정 및 기초 사용.",
    "crumbs": [
      "Assignments",
      "Environment setup"
    ]
  },
  {
    "objectID": "exercises/lab12-decision-tree.html",
    "href": "exercises/lab12-decision-tree.html",
    "title": "Decision Tree",
    "section": "",
    "text": "Gini/Entropy 기반 split 로직 Numpy 구현, 트리 깊이별 decision boundary 시각화 및 분산(instability) 실험.",
    "crumbs": [
      "Exercises",
      "Decision Tree"
    ]
  },
  {
    "objectID": "exercises/lab10-transfer-learning.html",
    "href": "exercises/lab10-transfer-learning.html",
    "title": "Transfer Learning",
    "section": "",
    "text": "Pretrained ResNet feature extractor vs fine-tuning 성능 비교, Grad-CAM으로 모델 attention 영역 시각화.",
    "crumbs": [
      "Exercises",
      "Transfer Learning"
    ]
  },
  {
    "objectID": "exercises/lab02-linear-logistic-regression.html",
    "href": "exercises/lab02-linear-logistic-regression.html",
    "title": "Linear & Logistic Regression",
    "section": "",
    "text": "퍼셉트론으로 AND/OR/XOR 게이트 실험, decision boundary 시각화.",
    "crumbs": [
      "Exercises",
      "Linear & Logistic Regression"
    ]
  },
  {
    "objectID": "exercises/lab05-numpy-mlp-to-pytorch.html",
    "href": "exercises/lab05-numpy-mlp-to-pytorch.html",
    "title": "Numpy MLP to PyTorch",
    "section": "",
    "text": "Dataset/DataLoader 구성, TensorBoard 로깅까지 동일 task 재현.",
    "crumbs": [
      "Exercises",
      "Numpy MLP to PyTorch"
    ]
  },
  {
    "objectID": "exercises/lab07-feature-map-visualization.html",
    "href": "exercises/lab07-feature-map-visualization.html",
    "title": "Feature map 시각화",
    "section": "",
    "text": "CIFAR-10 간단 CNN 학습 후 각 layer의 activation map과 학습된 filter 시각화, 파라미터 수 직접 계산 검증.",
    "crumbs": [
      "Exercises",
      "Feature map 시각화"
    ]
  },
  {
    "objectID": "exercises/index.html",
    "href": "exercises/index.html",
    "title": "Exercises",
    "section": "",
    "text": "실습 자료 목록입니다. 왼쪽 메뉴에서 항목을 선택하세요.",
    "crumbs": [
      "Exercises"
    ]
  },
  {
    "objectID": "exercises/lab04-mlp-forward-backward.html",
    "href": "exercises/lab04-mlp-forward-backward.html",
    "title": "MLP Forward/Backward",
    "section": "",
    "text": "손으로 gradient 계산 후 numerical gradient checking, 활성함수별 gradient flow 시각화.",
    "crumbs": [
      "Exercises",
      "MLP Forward/Backward"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning Course",
    "section": "",
    "text": "Welcome to the Machine Learning Course.\n\nExercises: 실습 자료\nAssignments: 과제"
  },
  {
    "objectID": "exercises/lab08-char-language-model.html",
    "href": "exercises/lab08-char-language-model.html",
    "title": "Char-level language model",
    "section": "",
    "text": "Vanilla RNN vs LSTM으로 텍스트 생성, hidden state 변화 시각화 및 gradient norm 추적(vanishing gradient 관찰).",
    "crumbs": [
      "Exercises",
      "Char-level language model"
    ]
  },
  {
    "objectID": "exercises/lab09-vae-latent-space.html",
    "href": "exercises/lab09-vae-latent-space.html",
    "title": "VAE latent space",
    "section": "",
    "text": "MNIST VAE 학습 후 2D latent space interpolation, GAN 학습 시 Generator/Discriminator loss 다이나믹스 관찰.",
    "crumbs": [
      "Exercises",
      "VAE latent space"
    ]
  },
  {
    "objectID": "exercises/lab13-bagging-vs-boosting.html",
    "href": "exercises/lab13-bagging-vs-boosting.html",
    "title": "Bagging vs Boosting",
    "section": "",
    "text": "단일 트리 vs Random Forest vs XGBoost의 bias-variance tradeoff 실험, feature importance 시각화.",
    "crumbs": [
      "Exercises",
      "Bagging vs Boosting"
    ]
  },
  {
    "objectID": "exercises/lab03-kfold-cv.html",
    "href": "exercises/lab03-kfold-cv.html",
    "title": "K-Fold Cross-Validation",
    "section": "",
    "text": "다항 회귀 차수별 train/val loss 곡선 그려보며 overfitting/underfitting 관찰.",
    "crumbs": [
      "Exercises",
      "K-Fold Cross-Validation"
    ]
  },
  {
    "objectID": "exercises/lab14-naive-bayes.html",
    "href": "exercises/lab14-naive-bayes.html",
    "title": "Naive Bayes",
    "section": "",
    "text": "Likelihood/prior/posterior 직접 계산, 조건부 독립 가정 위반 시 성능 변화 실험.",
    "crumbs": [
      "Exercises",
      "Naive Bayes"
    ]
  },
  {
    "objectID": "exercises/lab11-kernel-visualization.html",
    "href": "exercises/lab11-kernel-visualization.html",
    "title": "Kernel 시각화",
    "section": "",
    "text": "2D 데이터에서 linear/RBF/poly kernel의 decision boundary 비교, C·gamma 변화에 따른 margin 변화 관찰.",
    "crumbs": [
      "Exercises",
      "Kernel 시각화"
    ]
  },
  {
    "objectID": "exercises/lab01-numpy-basics.html",
    "href": "exercises/lab01-numpy-basics.html",
    "title": "Lab 01 - Numpy Basics",
    "section": "",
    "text": "NumPy 배열 생성, shape, dtype 다루기\nVectorization: 반복문 대신 배열 연산으로 빠르게 계산하기\nBroadcasting: 서로 다른 shape의 배열이 연산될 때의 규칙 이해하기",
    "crumbs": [
      "Exercises",
      "Lab 01 - Numpy Basics"
    ]
  },
  {
    "objectID": "exercises/lab01-numpy-basics.html#목표",
    "href": "exercises/lab01-numpy-basics.html#목표",
    "title": "Lab 01 - Numpy Basics",
    "section": "",
    "text": "NumPy 배열 생성, shape, dtype 다루기\nVectorization: 반복문 대신 배열 연산으로 빠르게 계산하기\nBroadcasting: 서로 다른 shape의 배열이 연산될 때의 규칙 이해하기",
    "crumbs": [
      "Exercises",
      "Lab 01 - Numpy Basics"
    ]
  },
  {
    "objectID": "exercises/lab01-numpy-basics.html#배열-생성-및-속성",
    "href": "exercises/lab01-numpy-basics.html#배열-생성-및-속성",
    "title": "Lab 01 - Numpy Basics",
    "section": "2 1. 배열 생성 및 속성",
    "text": "2 1. 배열 생성 및 속성\n\nimport numpy as np\n\n\n# 1차원 배열\na = np.array([1, 2, 3, 4, 5])\nprint(\"a =\", a)\nprint(\"shape:\", a.shape, \"  dtype:\", a.dtype)\n\na = [1 2 3 4 5]\nshape: (5,)   dtype: int64\n\n\n\n# 2차원 배열 (행렬)\nB = np.array([[1, 2], [3, 4], [5, 6]])\nprint(\"B =\\n\", B)\nprint(\"shape:\", B.shape)  # (행 개수, 열 개수)\n\nB =\n [[1 2]\n [3 4]\n [5 6]]\nshape: (3, 2)\n\n\n\n# 자주 쓰는 생성 함수\nprint(np.zeros(5))\nprint(np.ones((2, 3)))\nprint(np.arange(0, 10, 2))   # start, stop(미포함), step\nprint(np.linspace(0, 1, 5)) # 구간을 균등 분할\n\n[0. 0. 0. 0. 0.]\n[[1. 1. 1.]\n [1. 1. 1.]]\n[0 2 4 6 8]\n[0.   0.25 0.5  0.75 1.  ]",
    "crumbs": [
      "Exercises",
      "Lab 01 - Numpy Basics"
    ]
  },
  {
    "objectID": "exercises/lab01-numpy-basics.html#인덱싱과-슬라이싱",
    "href": "exercises/lab01-numpy-basics.html#인덱싱과-슬라이싱",
    "title": "Lab 01 - Numpy Basics",
    "section": "3 2. 인덱싱과 슬라이싱",
    "text": "3 2. 인덱싱과 슬라이싱\n\nx = np.array([10, 20, 30, 40, 50])\nprint(x[0], x[-1])      # 첫 원소, 마지막 원소\nprint(x[1:4])           # 인덱스 1, 2, 3\nprint(x[::2])            # 처음부터 끝까지 2칸씩\n\n10 50\n[20 30 40]\n[10 30 50]\n\n\n\nM = np.arange(12).reshape(3, 4)\nprint(\"M =\\n\", M)\nprint(\"M[1, 2] =\", M[1, 2])\nprint(\"2행 전체:\", M[2, :])\nprint(\"3열 전체:\", M[:, 3])\n\nM =\n [[ 0  1  2  3]\n [ 4  5  6  7]\n [ 8  9 10 11]]\nM[1, 2] = 6\n2행 전체: [ 8  9 10 11]\n3열 전체: [ 3  7 11]",
    "crumbs": [
      "Exercises",
      "Lab 01 - Numpy Basics"
    ]
  },
  {
    "objectID": "exercises/lab01-numpy-basics.html#vectorization-벡터화",
    "href": "exercises/lab01-numpy-basics.html#vectorization-벡터화",
    "title": "Lab 01 - Numpy Basics",
    "section": "4 3. Vectorization (벡터화)",
    "text": "4 3. Vectorization (벡터화)\n반복문 대신 배열 전체에 한 번에 연산을 적용하면 빠르고 코드도 짧아진다.\n\n# 비벡터화: for 루프\ndef sum_squares_loop(arr):\n    total = 0\n    for x in arr:\n        total += x ** 2\n    return total\n\n# 벡터화: 배열 연산\ndef sum_squares_vec(arr):\n    return (arr ** 2).sum()\n\n\nlarge = np.random.randn(100_000)\n%timeit sum_squares_loop(large)\n%timeit sum_squares_vec(large)\n\n8.76 ms ± 165 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n24.3 μs ± 98.5 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\n\n# 예: 시그모이드 σ(z) = 1 / (1 + exp(-z))\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n\nz = np.linspace(-5, 5, 11)\nprint(\"z     =\", z)\nprint(\"sigmoid(z) =\", sigmoid(z))\n\nz     = [-5. -4. -3. -2. -1.  0.  1.  2.  3.  4.  5.]\nsigmoid(z) = [0.00669285 0.01798621 0.04742587 0.11920292 0.26894142 0.5\n 0.73105858 0.88079708 0.95257413 0.98201379 0.99330715]",
    "crumbs": [
      "Exercises",
      "Lab 01 - Numpy Basics"
    ]
  },
  {
    "objectID": "exercises/lab01-numpy-basics.html#broadcasting",
    "href": "exercises/lab01-numpy-basics.html#broadcasting",
    "title": "Lab 01 - Numpy Basics",
    "section": "5 4. Broadcasting",
    "text": "5 4. Broadcasting\nshape가 다른 배열끼리 연산할 때, NumPy가 자동으로 확장하는 규칙.\n\n규칙: 차원을 뒤에서부터 맞춘다. 한쪽이 1이면 그 차원을 늘려서 맞춘다.\n예: (3, 4) + (4,) → (4,)를 (1, 4)로 보고 → (3, 4)와 맞춤.\n\n\n# (3, 4) + (4,) → 각 행에 같은 [1,2,3,4]가 더해짐\nA = np.arange(12).reshape(3, 4)\nb = np.array([1, 2, 3, 4])\nprint(\"A =\\n\", A)\nprint(\"b =\", b)\nprint(\"A + b =\\n\", A + b)\n\nA =\n [[ 0  1  2  3]\n [ 4  5  6  7]\n [ 8  9 10 11]]\nb = [1 2 3 4]\nA + b =\n [[ 1  3  5  7]\n [ 5  7  9 11]\n [ 9 11 13 15]]\n\n\n\n# (3, 4) + (3, 1) → 각 열에 같은 열 벡터가 더해짐\nc = np.array([[10], [20], [30]])\nprint(\"c =\\n\", c)\nprint(\"A + c =\\n\", A + c)\n\nc =\n [[10]\n [20]\n [30]]\nA + c =\n [[10 11 12 13]\n [24 25 26 27]\n [38 39 40 41]]\n\n\n\n# 정규화: 각 행을 그 행의 합으로 나누기 (broadcasting 활용)\nrow_sums = A.sum(axis=1, keepdims=True)  # shape (3, 1)\nprint(\"row_sums =\\n\", row_sums)\nnormalized = A / row_sums\nprint(\"행 합이 1이 되도록:\\n\", normalized)\nprint(\"각 행 합:\", normalized.sum(axis=1))\n\nrow_sums =\n [[ 6]\n [22]\n [38]]\n행 합이 1이 되도록:\n [[0.         0.16666667 0.33333333 0.5       ]\n [0.18181818 0.22727273 0.27272727 0.31818182]\n [0.21052632 0.23684211 0.26315789 0.28947368]]\n각 행 합: [1. 1. 1.]",
    "crumbs": [
      "Exercises",
      "Lab 01 - Numpy Basics"
    ]
  },
  {
    "objectID": "exercises/lab01-numpy-basics.html#연습",
    "href": "exercises/lab01-numpy-basics.html#연습",
    "title": "Lab 01 - Numpy Basics",
    "section": "6 5. 연습",
    "text": "6 5. 연습\n벡터 v와 각 행이 v와 같은 행렬 M (shape (4, 3))을 만드세요.\n(힌트: np.tile 또는 broadcasting을 이용할 수 있다.)\n\nv = np.array([1, 2, 3])\n# M = ???  # shape (4, 3), 각 행이 [1,2,3]\n# print(M)\n\n\n# 풀이 예시 1: tile\nM1 = np.tile(v, (4, 1))\nprint(\"tile:\\n\", M1)\n\n# 풀이 예시 2: broadcasting\nM2 = v + np.zeros((4, 3))\nprint(\"broadcasting:\\n\", M2)\n\ntile:\n [[1 2 3]\n [1 2 3]\n [1 2 3]\n [1 2 3]]\nbroadcasting:\n [[1. 2. 3.]\n [1. 2. 3.]\n [1. 2. 3.]\n [1. 2. 3.]]",
    "crumbs": [
      "Exercises",
      "Lab 01 - Numpy Basics"
    ]
  },
  {
    "objectID": "exercises/lab01-numpy-basics.html#참고",
    "href": "exercises/lab01-numpy-basics.html#참고",
    "title": "Lab 01 - Numpy Basics",
    "section": "7 참고",
    "text": "7 참고\n\nML에서 데이터는 보통 (샘플 수, 특성 수) shape의 2차원 배열로 다룬다.\nGradient, loss 계산 등은 벡터화·브로드캐스팅을 쓰면 구현이 단순해지고 속도도 빨라진다.",
    "crumbs": [
      "Exercises",
      "Lab 01 - Numpy Basics"
    ]
  },
  {
    "objectID": "exercises/lab06-optimizer-comparison.html",
    "href": "exercises/lab06-optimizer-comparison.html",
    "title": "Optimizer 비교",
    "section": "",
    "text": "SGD/Momentum/Adam의 loss surface 위 궤적 시각화, Dropout·weight decay·BatchNorm 유무에 따른 학습 곡선 비교.",
    "crumbs": [
      "Exercises",
      "Optimizer 비교"
    ]
  },
  {
    "objectID": "assignments/hw07-svm-kernel.html",
    "href": "assignments/hw07-svm-kernel.html",
    "title": "SVM & Kernel",
    "section": "",
    "text": "SVM 및 Kernel 기법 실습.",
    "crumbs": [
      "Assignments",
      "SVM & Kernel"
    ]
  },
  {
    "objectID": "assignments/hw02-simple-regression.html",
    "href": "assignments/hw02-simple-regression.html",
    "title": "Simple regression",
    "section": "",
    "text": "Numpy로 매우 단순한 회귀 모델 구현.",
    "crumbs": [
      "Assignments",
      "Simple regression"
    ]
  },
  {
    "objectID": "assignments/hw05-pytorch-rnn.html",
    "href": "assignments/hw05-pytorch-rnn.html",
    "title": "RNN",
    "section": "",
    "text": "PyTorch로 recurrent neural network 구현.",
    "crumbs": [
      "Assignments",
      "RNN"
    ]
  },
  {
    "objectID": "assignments/hw06-autoencoder.html",
    "href": "assignments/hw06-autoencoder.html",
    "title": "Autoencoder",
    "section": "",
    "text": "PyTorch Autoencoder 구현.",
    "crumbs": [
      "Assignments",
      "Autoencoder"
    ]
  }
]