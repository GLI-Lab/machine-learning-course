# Lab 01 - NumPy Basics for Machine Learning

[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/GLI-Lab/machine-learning-course/blob/students/exercises/lab01/numpy-basics.ipynb)

## Objectives

- Understand the basic concepts of NumPy arrays: creation, `shape`, and `dtype`.
- **Vectorization**: Learn how to replace slow Python loops with fast, C-optimized array operations.
- **Broadcasting**: Grasp the rules NumPy uses to perform operations on arrays of different shapes.

> #### Basic Concept: Why NumPy?
> In Machine Learning, we deal with massive amounts of data (vectors, matrices, tensors). Standard Python lists are too slow and memory-inefficient for these calculations. NumPy arrays are stored in contiguous blocks of memory and operations are implemented in highly optimized C code, making mathematical operations drastically faster. NumPy is the foundational library that modern deep learning frameworks (like PyTorch and TensorFlow) are built upon.



## 1. Array Creation and Attributes

Before we manipulate data, we need to know how to create arrays and inspect their properties.



```{python}
import numpy as np

```

```{python}
# 1-Dimensional Array (Vector)
a = np.array([1, 2, 3, 4, 5])
print("a =", a)
print("shape:", a.shape) # Returns a tuple representing the size of each dimension
print("dtype:", a.dtype) # Data type of the elements

```

```{python}
# 2-Dimensional Array (Matrix)
B = np.array([[1, 2], [3, 4], [5, 6]])
print("B =\n", B)
print("shape:", B.shape)  # (number of rows, number of columns)

```

> #### Caveat: Data Types (dtype)
>
> By default, NumPy might create arrays with `float64` or `int64` depending on your OS and the input. In deep learning, memory is expensive, so we often explicitly cast arrays to 32-bit floats using `a.astype(np.float32)`.



```{python}
# Frequently used creation functions
print("Zeros:\n", np.zeros(5))               # Array of zeros
print("Ones:\n", np.ones((2, 3)))            # 2x3 matrix of ones
print("Arange:\n", np.arange(0, 10, 2))      # start, stop (exclusive), step
print("Linspace:\n", np.linspace(0, 1, 5))   # 5 evenly spaced numbers between 0 and 1 (inclusive)

```


## 2. Indexing and Slicing

Accessing elements in NumPy is similar to standard Python lists but extends to multiple dimensions.

```{python}
x = np.array([10, 20, 30, 40, 50])
print("First element:", x[0])
print("Last element:", x[-1])
print("Slice [1:4]:", x[1:4])     # Indices 1, 2, 3 (exclusive of 4)
print("Step slice [::2]:", x[::2]) # Every 2nd element from start to end

```

```{python}
M = np.arange(12).reshape(3, 4)
print("M =\n", M)
print("Specific element M[1, 2]:", M[1, 2]) # Row 1, Column 2 (0-indexed)
print("Entire 2nd row:", M[2, :])
print("Entire 3rd column:", M[:, 3])

```

> #### Caveat: Slices are Views, Not Copies!
>
> When you slice an array in NumPy, it returns a **view** of the original memory, not a new array. If you modify the slice, the original array is modified too. If you need an independent copy, explicitly use `.copy()`.
>
> ```python
> x_slice = x[1:4].copy() 
>
> ```



## 3. Vectorization

Vectorization is the process of applying an operation to an entire array at once, rather than iterating through its elements using a `for` loop. This results in cleaner code and massive performance boosts.

```{python}
# Non-vectorized approach: Python 'for' loop
def sum_squares_loop(arr):
    total = 0
    for x in arr:
        total += x ** 2
    return total

# Vectorized approach: NumPy array operation
def sum_squares_vec(arr):
    return (arr ** 2).sum()

```

Let's test the speed difference:

```{python}
large = np.random.randn(100_000)
print("Loop time:")
%timeit sum_squares_loop(large)
print("Vectorized time:")
%timeit sum_squares_vec(large)

```

Vectorization makes applying mathematical functions to arrays trivial. For example, implementing the Sigmoid activation function $\sigma(z) = \frac{1}{1 + e^{-z}}$:

```{python}
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

z = np.linspace(-5, 5, 11)
print("z          =", z)
print("sigmoid(z) =", sigmoid(z))

```

## 4. Broadcasting

Broadcasting is a powerful mechanism that allows NumPy to perform operations on arrays of different shapes.

**The Rule of Broadcasting:** NumPy compares array shapes from right to left. Two dimensions are compatible if:

1. They are equal, or
2. One of them is 1.
If an array has fewer dimensions, a dimension of `1` is implicitly prepended to its shape.

```{python}
# Shape (3, 4) + Shape (4,) 
# The (4,) is treated as (1, 4), then copied across 3 rows to match (3, 4)
A = np.arange(12).reshape(3, 4)
b = np.array([1, 2, 3, 4])
print("A =\n", A)
print("b =", b)
print("A + b =\n", A + b)

```

```{python}
# Shape (3, 4) + Shape (3, 1)
# The (3, 1) array is copied across 4 columns to match (3, 4)
c = np.array([[10], [20], [30]])
print("c =\n", c)
print("A + c =\n", A + c)

```

**Practical Example: Normalizing Data**
A very common task in ML is normalizing data (e.g., making rows sum to 1).

```{python}
# keepdims=True ensures the shape remains (3, 1) instead of dropping to (3,)
row_sums = A.sum(axis=1, keepdims=True)  
print("row_sums (shape", row_sums.shape, ") =\n", row_sums)

normalized = A / row_sums
print("Normalized A (rows sum to 1):\n", normalized)
print("Verify row sums:", normalized.sum(axis=1))

```

> #### Caveat: The Danger of Rank-1 Arrays
>
> Arrays with shape `(N,)` are called Rank-1 arrays (e.g., `b` in the example above). They do not behave like traditional row or column vectors and can cause confusing broadcasting bugs.
> **Best Practice in ML:** Always reshape your 1D vectors into explicit 2D row matrices `(1, N)` or column matrices `(N, 1)` using `.reshape(-1, 1)` or `np.newaxis` to avoid unintended broadcasting behaviors.



## 5. Exercises

Create a vector `v` and a matrix `M` with shape `(4, 3)` where every row of `M` is exactly `v`.
*(Hint: You can achieve this using `np.tile` or by utilizing broadcasting).*

```{python}
v = np.array([1, 2, 3])
# M = ???  # Desired shape (4, 3), each row is [1, 2, 3]
# print(M)

```

**Solutions:**

```{python}
# Solution 1: Using np.tile
M1 = np.tile(v, (4, 1))
print("Using tile:\n", M1)

# Solution 2: Using broadcasting
# Adding a (3,) vector to a (4, 3) matrix of zeros automatically broadcasts it.
M2 = v + np.zeros((4, 3))
print("Using broadcasting:\n", M2)

```

## Summary for Machine Learning

* In ML, our datasets are usually represented as 2D arrays with the shape `(num_samples, num_features)`.
* When calculating gradients, losses, or passing data through neural network layers, **never** use Python loops. Always use **Vectorization** and **Broadcasting**. It will make your code shorter, significantly faster, and easier to read.

```

Would you like me to add a section specifically covering Matrix Multiplication (`np.dot` vs `@`), as that is also a crucial NumPy concept for Machine Learning?

```