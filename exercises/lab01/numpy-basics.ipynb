{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 01 - NumPy Basics for Machine Learning\n",
        "\n",
        "[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/GLI-Lab/machine-learning-course/blob/students/exercises/lab01/numpy-basics.ipynb)\n",
        "\n",
        "## Objectives\n",
        "\n",
        "- Understand the basic concepts of NumPy arrays: creation, `shape`, and `dtype`.\n",
        "- **Vectorization**: Learn how to replace slow Python loops with fast, C-optimized array operations.\n",
        "- **Broadcasting**: Grasp the rules NumPy uses to perform operations on arrays of different shapes.\n",
        "\n",
        "> #### ðŸ“ Basic Concept: Why NumPy?\n",
        "> In Machine Learning, we deal with massive amounts of data (vectors, matrices, tensors). Standard Python lists are too slow and memory-inefficient for these calculations. NumPy arrays are stored in contiguous blocks of memory and operations are implemented in highly optimized C code, making mathematical operations drastically faster. NumPy is the foundational library that modern deep learning frameworks (like PyTorch and TensorFlow) are built upon.\n",
        "\n",
        "## 1. Array Creation and Attributes\n",
        "\n",
        "Before we manipulate data, we need to know how to create arrays and inspect their properties.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1-Dimensional Array (Vector)\n",
        "a = np.array([1, 2, 3, 4, 5])\n",
        "print(\"a =\", a)\n",
        "print(\"shape:\", a.shape) # Returns a tuple representing the size of each dimension\n",
        "print(\"dtype:\", a.dtype) # Data type of the elements"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 2-Dimensional Array (Matrix)\n",
        "B = np.array([[1, 2], [3, 4], [5, 6]])\n",
        "print(\"B =\\n\", B)\n",
        "print(\"shape:\", B.shape)  # (number of rows, number of columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> #### âš ï¸ Caveat: Data Types (dtype)\n",
        ">\n",
        "> By default, NumPy might create arrays with `float64` or `int64` depending on your OS and the input. In deep learning, memory is expensive, so we often explicitly cast arrays to 32-bit floats using `a.astype(np.float32)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Frequently used creation functions\n",
        "print(\"Zeros:\\n\", np.zeros(5))               # Array of zeros\n",
        "print(\"Ones:\\n\", np.ones((2, 3)))            # 2x3 matrix of ones\n",
        "print(\"Arange:\\n\", np.arange(0, 10, 2))      # start, stop (exclusive), step\n",
        "print(\"Linspace:\\n\", np.linspace(0, 1, 5))   # 5 evenly spaced numbers between 0 and 1 (inclusive)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Indexing and Slicing\n",
        "\n",
        "Accessing elements in NumPy is similar to standard Python lists but extends to multiple dimensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "x = np.array([10, 20, 30, 40, 50])\n",
        "print(\"First element:\", x[0])\n",
        "print(\"Last element:\", x[-1])\n",
        "print(\"Slice [1:4]:\", x[1:4])     # Indices 1, 2, 3 (exclusive of 4)\n",
        "print(\"Step slice [::2]:\", x[::2]) # Every 2nd element from start to end"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "M = np.arange(12).reshape(3, 4)\n",
        "print(\"M =\\n\", M)\n",
        "print(\"Specific element M[1, 2]:\", M[1, 2]) # Row 1, Column 2 (0-indexed)\n",
        "print(\"Entire 2nd row:\", M[2, :])\n",
        "print(\"Entire 3rd column:\", M[:, 3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> #### âš ï¸ Caveat: Slices are Views, Not Copies!\n",
        ">\n",
        "> When you slice an array in NumPy, it returns a **view** of the original memory, not a new array. If you modify the slice, the original array is modified too. If you need an independent copy, explicitly use `.copy()`.\n",
        ">\n",
        "> ```python\n",
        "> x_slice = x[1:4].copy() \n",
        ">\n",
        "> ```\n",
        "\n",
        "## 3. Vectorization\n",
        "\n",
        "Vectorization is the process of applying an operation to an entire array at once, rather than iterating through its elements using a `for` loop. This results in cleaner code and massive performance boosts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Non-vectorized approach: Python 'for' loop\n",
        "def sum_squares_loop(arr):\n",
        "    total = 0\n",
        "    for x in arr:\n",
        "        total += x ** 2\n",
        "    return total\n",
        "\n",
        "# Vectorized approach: NumPy array operation\n",
        "def sum_squares_vec(arr):\n",
        "    return (arr ** 2).sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's test the speed difference:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "large = np.random.randn(100_000)\n",
        "print(\"Loop time:\")\n",
        "%timeit sum_squares_loop(large)\n",
        "print(\"Vectorized time:\")\n",
        "%timeit sum_squares_vec(large)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vectorization makes applying mathematical functions to arrays trivial. For example, implementing the Sigmoid activation function $\\sigma(z) = \\frac{1}{1 + e^{-z}}$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "z = np.linspace(-5, 5, 11)\n",
        "print(\"z          =\", z)\n",
        "print(\"sigmoid(z) =\", sigmoid(z))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Broadcasting\n",
        "\n",
        "Broadcasting is a powerful mechanism that allows NumPy to perform operations on arrays of different shapes.\n",
        "\n",
        "**The Rule of Broadcasting:** NumPy compares array shapes from right to left. Two dimensions are compatible if:\n",
        "\n",
        "1. They are equal, or\n",
        "2. One of them is 1.\n",
        "If an array has fewer dimensions, a dimension of `1` is implicitly prepended to its shape."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Shape (3, 4) + Shape (4,) \n",
        "# The (4,) is treated as (1, 4), then copied across 3 rows to match (3, 4)\n",
        "A = np.arange(12).reshape(3, 4)\n",
        "b = np.array([1, 2, 3, 4])\n",
        "print(\"A =\\n\", A)\n",
        "print(\"b =\", b)\n",
        "print(\"A + b =\\n\", A + b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Shape (3, 4) + Shape (3, 1)\n",
        "# The (3, 1) array is copied across 4 columns to match (3, 4)\n",
        "c = np.array([[10], [20], [30]])\n",
        "print(\"c =\\n\", c)\n",
        "print(\"A + c =\\n\", A + c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Practical Example: Normalizing Data**\n",
        "A very common task in ML is normalizing data (e.g., making rows sum to 1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# keepdims=True ensures the shape remains (3, 1) instead of dropping to (3,)\n",
        "row_sums = A.sum(axis=1, keepdims=True)  \n",
        "print(\"row_sums (shape\", row_sums.shape, \") =\\n\", row_sums)\n",
        "\n",
        "normalized = A / row_sums\n",
        "print(\"Normalized A (rows sum to 1):\\n\", normalized)\n",
        "print(\"Verify row sums:\", normalized.sum(axis=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> #### âš ï¸ Caveat: The Danger of Rank-1 Arrays\n",
        ">\n",
        "> Arrays with shape `(N,)` are called Rank-1 arrays (e.g., `b` in the example above). They do not behave like traditional row or column vectors and can cause confusing broadcasting bugs.\n",
        "> **Best Practice in ML:** Always reshape your 1D vectors into explicit 2D row matrices `(1, N)` or column matrices `(N, 1)` using `.reshape(-1, 1)` or `np.newaxis` to avoid unintended broadcasting behaviors.\n",
        "\n",
        "## 5. Exercises\n",
        "\n",
        "Create a vector `v` and a matrix `M` with shape `(4, 3)` where every row of `M` is exactly `v`.\n",
        "*(Hint: You can achieve this using `np.tile` or by utilizing broadcasting).*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "v = np.array([1, 2, 3])\n",
        "# M = ???  # Desired shape (4, 3), each row is [1, 2, 3]\n",
        "# print(M)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Solutions:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Solution 1: Using np.tile\n",
        "M1 = np.tile(v, (4, 1))\n",
        "print(\"Using tile:\\n\", M1)\n",
        "\n",
        "# Solution 2: Using broadcasting\n",
        "# Adding a (3,) vector to a (4, 3) matrix of zeros automatically broadcasts it.\n",
        "M2 = v + np.zeros((4, 3))\n",
        "print(\"Using broadcasting:\\n\", M2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary for Machine Learning\n",
        "\n",
        "* In ML, our datasets are usually represented as 2D arrays with the shape `(num_samples, num_features)`.\n",
        "* When calculating gradients, losses, or passing data through neural network layers, **never** use Python loops. Always use **Vectorization** and **Broadcasting**. It will make your code shorter, significantly faster, and easier to read.\n",
        "\n",
        "```\n",
        "\n",
        "Would you like me to add a section specifically covering Matrix Multiplication (`np.dot` vs `@`), as that is also a crucial NumPy concept for Machine Learning?\n",
        "\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}