{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Lab 02 - Linear & Logistic Regression\"\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "## 목표\n",
        "\n",
        "- **선형 회귀**: 최소제곱법으로 직선 피팅하기\n",
        "- **퍼셉트론**: AND / OR / XOR 게이트 실험 (선형 분리 가능 vs 불가능)\n",
        "- **로지스틱 회귀**: 시그모이드, 결정 경계(decision boundary) 시각화\n",
        "\n",
        "---\n",
        "\n",
        "## 1. 선형 회귀 (1변수)\n",
        "\n",
        "$y \\approx w_0 + w_1 x$ 형태. 정규방정식 $\\hat{w} = (X^T X)^{-1} X^T y$ 로 한 번에 구할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 데이터: y = 2 + 3x + 노이즈\n",
        "np.random.seed(42)\n",
        "n = 30\n",
        "x = np.linspace(0, 2, n)\n",
        "y = 2 + 3 * x + 0.5 * np.random.randn(n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 설계 행렬 X: 각 행이 [1, x_i]\n",
        "X = np.column_stack([np.ones(n), x])\n",
        "w_hat = np.linalg.solve(X.T @ X, X.T @ y)\n",
        "print(\"w_0 =\", w_hat[0], \", w_1 =\", w_hat[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.scatter(x, y, label=\"data\")\n",
        "x_line = np.linspace(0, 2, 100)\n",
        "y_line = w_hat[0] + w_hat[1] * x_line\n",
        "plt.plot(x_line, y_line, \"r-\", label=\"fitted\")\n",
        "plt.xlabel(\"x\"); plt.ylabel(\"y\"); plt.legend(); plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2. 퍼셉트론 (AND, OR, XOR)\n",
        "\n",
        "퍼셉트론: $f(x) = \\mathbb{1}[ w^T x + b > 0 ]$. 선형 분리 가능한 문제만 풀 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# AND, OR, XOR 진리표 (입력 2개)\n",
        "X_gate = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y_and = np.array([0, 0, 0, 1])\n",
        "y_or  = np.array([0, 1, 1, 1])\n",
        "y_xor = np.array([0, 1, 1, 0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def perceptron_predict(X, w, b):\n",
        "    \"\"\" X: (n, d), w: (d,), b: scalar \"\"\"\n",
        "    return (X @ w + b > 0).astype(int)\n",
        "\n",
        "def fit_perceptron(X, y, lr=0.1, epochs=100):\n",
        "    n, d = X.shape\n",
        "    w, b = np.zeros(d), 0.0\n",
        "    for _ in range(epochs):\n",
        "        pred = perceptron_predict(X, w, b)\n",
        "        err = y - pred\n",
        "        w += lr * (X.T @ err)\n",
        "        b += lr * err.sum()\n",
        "    return w, b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "w_and, b_and = fit_perceptron(X_gate, y_and)\n",
        "w_or,  b_or  = fit_perceptron(X_gate, y_or)\n",
        "print(\"AND 예측:\", perceptron_predict(X_gate, w_and, b_and), \" (정답:\", y_and, \")\")\n",
        "print(\"OR  예측:\", perceptron_predict(X_gate, w_or,  b_or),  \" (정답:\", y_or, \")\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# XOR: 선형 분리 불가능 → 퍼셉트론 한 개로는 학습 실패\n",
        "w_xor, b_xor = fit_perceptron(X_gate, y_xor)\n",
        "print(\"XOR 예측:\", perceptron_predict(X_gate, w_xor, b_xor), \" (정답:\", y_xor, \")\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 3. 결정 경계 시각화 (AND / OR)\n",
        "\n",
        "$w_1 x_1 + w_2 x_2 + b = 0$ 이 결정 경계(직선)이다. $x_2 = -(w_1/w_2)x_1 - b/w_2$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def plot_decision_boundary_2d(X, y, w, b, title=\"\"):\n",
        "    plt.scatter(X[y==0, 0], X[y==0, 1], marker=\"o\", label=\"0\")\n",
        "    plt.scatter(X[y==1, 0], X[y==1, 1], marker=\"s\", label=\"1\")\n",
        "    x1 = np.array([-0.5, 1.5])\n",
        "    if np.abs(w[1]) > 1e-8:\n",
        "        x2 = -(w[0] * x1 + b) / w[1]\n",
        "        plt.plot(x1, x2, \"k-\", label=\"decision boundary\")\n",
        "    plt.xlabel(\"$x_1$\"); plt.ylabel(\"$x_2$\"); plt.legend(); plt.title(title); plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot_decision_boundary_2d(X_gate, y_and, w_and, b_and, \"AND\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot_decision_boundary_2d(X_gate, y_or, w_or, b_or, \"OR\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 4. 로지스틱 회귀 (결정 경계)\n",
        "\n",
        "로지스틱 회귀: $P(y=1|x) = \\sigma(w^T x + b)$, $\\sigma(z) = 1/(1+e^{-z})$.  \n",
        "결정 경계는 $\\sigma=0.5$ 인 곳, 즉 $w^T x + b = 0$ (직선)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
        "\n",
        "# 2D 예시 데이터: 두 클래스가 약간 겹침\n",
        "np.random.seed(1)\n",
        "n1, n2 = 25, 25\n",
        "X1 = np.random.randn(n1, 2) + np.array([1, 1])\n",
        "X2 = np.random.randn(n2, 2) + np.array([-1, -1])\n",
        "X_lr = np.vstack([X1, X2])\n",
        "y_lr = np.array([0] * n1 + [1] * n2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 간단한 경사 하강으로 w, b 추정 (음의 로그우도 최소화)\n",
        "def fit_logistic_simple(X, y, lr=0.1, epochs=2000):\n",
        "    n, d = X.shape\n",
        "    w, b = np.zeros(d), 0.0\n",
        "    for _ in range(epochs):\n",
        "        logit = X @ w + b\n",
        "        p = sigmoid(logit)\n",
        "        grad_w = X.T @ (p - y)\n",
        "        grad_b = (p - y).sum()\n",
        "        w -= lr * grad_w\n",
        "        b -= lr * grad_b\n",
        "    return w, b\n",
        "\n",
        "w_lr, b_lr = fit_logistic_simple(X_lr, y_lr)\n",
        "pred_lr = (sigmoid(X_lr @ w_lr + b_lr) >= 0.5).astype(int)\n",
        "print(\"정확도:\", (pred_lr == y_lr).mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 결정 경계 그리기: w1*x1 + w2*x2 + b = 0  →  x2 = -(w1*x1 + b)/w2\n",
        "plt.scatter(X_lr[y_lr==0, 0], X_lr[y_lr==0, 1], marker=\"o\", label=\"0\")\n",
        "plt.scatter(X_lr[y_lr==1, 0], X_lr[y_lr==1, 1], marker=\"s\", label=\"1\")\n",
        "x1_min, x1_max = X_lr[:, 0].min() - 0.5, X_lr[:, 0].max() + 0.5\n",
        "x1_grid = np.linspace(x1_min, x1_max, 100)\n",
        "if np.abs(w_lr[1]) > 1e-8:\n",
        "    x2_boundary = -(w_lr[0] * x1_grid + b_lr) / w_lr[1]\n",
        "    plt.plot(x1_grid, x2_boundary, \"k-\", lw=2, label=\"decision boundary\")\n",
        "plt.xlabel(\"$x_1$\"); plt.ylabel(\"$x_2$\"); plt.legend(); plt.title(\"Logistic regression\"); plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 5. 연습\n",
        "\n",
        "- XOR 데이터에 대해 **두 개의 퍼셉트론**을 써서 해결하는 방법을 생각해 보세요.  \n",
        "  (힌트: 은닉층 하나를 두고 AND(OR, OR) 같은 구조.)\n",
        "- 위 로지스틱 회귀에서 `epochs`를 100으로 줄이면 결정 경계가 어떻게 달라지는지 확인해 보세요.\n",
        "\n",
        "---\n",
        "\n",
        "## 참고\n",
        "\n",
        "- 퍼셉트론 하나 = 선형 결정 경계만 가능. XOR은 다층 퍼셉트론(MLP)으로 해결.\n",
        "- 로지스틱 회귀의 결정 경계도 선형; 비선형 경계가 필요하면 특성 추가 또는 신경망을 사용한다."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}